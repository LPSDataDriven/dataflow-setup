# ğŸš€ Guia Completo de ImplementaÃ§Ã£o em ProduÃ§Ã£o

## GitHub + Airflow + DBT Core + AWS (ECR, S3)

Este guia mostra como implementar um pipeline de dados completo usando:
- **GitHub** para versionamento
- **Airflow** com Docker para orquestraÃ§Ã£o
- **DBT Core** para transformaÃ§Ãµes
- **AWS ECR** para registry de containers (praticamente gratuito no free tier)
- **AWS S3** para artefatos/logs
- **Snowflake** como data warehouse

## ğŸ“‹ VisÃ£o Geral da Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Repo   â”‚â”€â”€â”€â–¶â”‚   GitHub Actionsâ”‚â”€â”€â”€â–¶â”‚   AWS ECR       â”‚
â”‚                 â”‚    â”‚   (CI/CD)       â”‚    â”‚   (Images)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket     â”‚â—€â”€â”€â”€â”‚   Airflow        â”‚â”€â”€â”€â–¶â”‚   DBT Core      â”‚
â”‚   (Artifacts)   â”‚    â”‚   (Local/EC2)    â”‚    â”‚   (Transform)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚   Snowflake     â”‚
                                                  â”‚   (Data Lake)   â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Fluxo Completo de ProduÃ§Ã£o (End-to-End)

Este fluxo garante que todas as alteraÃ§Ãµes passem por validaÃ§Ã£o, build automatizado e deploy na EC2.

### VisÃ£o Geral do Fluxo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Desenvolvimento â”‚
â”‚    Local          â”‚
â”‚    (feature/*)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ git push
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Pull Request  â”‚
â”‚    â†’ develop     â”‚
â”‚    (validaÃ§Ã£o)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ merge
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Merge develop â”‚
â”‚    â†’ GitHub      â”‚
â”‚    Actions       â”‚
â”‚    (build ECR)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ECR Registry  â”‚
â”‚    (imagem)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. EC2 Update    â”‚
â”‚    (git pull +   â”‚
â”‚     docker pull) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. **Desenvolvimento Local**

```bash
# 1. Criar branch de feature
git checkout -b feature/new-pipeline

# 2. Fazer alteraÃ§Ãµes (DAGs, modelos DBT, etc.)
# Editar arquivos em:
# - airflow/dags/*.py
# - dbt/models/*.sql
# - pyproject.toml (dependÃªncias)

# 3. Testar localmente
docker-compose build
docker-compose up -d
# Acessar: http://localhost:8080

# 4. Validar cÃ³digo localmente (opcional, mas recomendado)
pre-commit run --all-files

# 5. Commit e push
git add .
git commit -m "Add new DBT pipeline"
git push origin feature/new-pipeline
```

**Importante**: As alteraÃ§Ãµes locais **nÃ£o** aparecem automaticamente na EC2. Elas precisam passar pelo fluxo completo.

### 2. **Pull Request para `develop`**

```bash
# 1. Criar PR no GitHub (feature/new-pipeline â†’ develop)
# 2. GitHub Actions executa automaticamente:
#    - ValidaÃ§Ãµes (pre-commit hooks)
#    - Lint de cÃ³digo Python
#    - Lint de SQL (sqlfluff)
#    - ValidaÃ§Ã£o de sintaxe dos DAGs
```

**O que acontece**:
- âœ… Workflow `.github/workflows/lint_on_push.yml` executa
- âœ… Valida apenas arquivos modificados no PR
- âœ… Se passar, PR pode ser mergeado

### 3. **Merge para `develop` â†’ Build AutomÃ¡tico**

Quando vocÃª faz merge do PR para `develop`:

```bash
# 1. Merge PR no GitHub (feature/new-pipeline â†’ develop)
# 2. GitHub Actions executa automaticamente:
#    - Workflow: .github/workflows/build-and-push-ecr.yml
#    - Build da imagem Docker
#    - Push para ECR com tag: develop
#    - Push tambÃ©m com tag: <SHA do commit>
```

**O que acontece**:
- âœ… Workflow `.github/workflows/build-and-push-ecr.yml` executa
- âœ… Build da imagem Docker usando `airflow/Dockerfile`
- âœ… Push para ECR: `679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:develop`
- âœ… Push tambÃ©m com SHA: `679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:<SHA>`

**Verificar o build**:
- VÃ¡ em **Actions** no GitHub â†’ veja o workflow "Build and Push to ECR"
- Se falhar, veja os logs para identificar o problema

### 4. **Merge `develop` â†’ `main` â†’ Build para ProduÃ§Ã£o**

Quando vocÃª faz merge de `develop` para `main`:

```bash
# 1. Merge develop â†’ main no GitHub
# 2. GitHub Actions executa automaticamente:
#    - Build da imagem Docker
#    - Push para ECR com tags: main, latest, <SHA>
```

**O que acontece**:
- âœ… Build da imagem Docker
- âœ… Push para ECR: `679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:main`
- âœ… Push tambÃ©m: `679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:latest`
- âœ… Push com SHA: `679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:<SHA>`

### 5. **Atualizar EC2 com CÃ³digo e Imagem**

**IMPORTANTE**: A EC2 precisa ser atualizada manualmente (ou via script automatizado) apÃ³s o merge. O cÃ³digo e a imagem **nÃ£o** sÃ£o atualizados automaticamente.

#### 5.1 Atualizar CÃ³digo na EC2

Na EC2, vocÃª precisa fazer `git pull` para pegar as alteraÃ§Ãµes mais recentes:

```bash
# 1. Conectar na EC2
ssh -i ~/.ssh/airflow-ec2.pem ec2-user@<IP-EC2>

# 2. Ir para o diretÃ³rio do projeto
cd ~/dataflow-setup

# 3. Verificar branch atual (deve ser main ou develop)
git branch

# 4. Atualizar cÃ³digo da branch
git pull origin main  # ou develop, dependendo do que vocÃª quer

# 5. Verificar se hÃ¡ alteraÃ§Ãµes
git log --oneline -5
```

**Por que isso Ã© necessÃ¡rio?**
- O `docker-compose.yml` monta volumes do filesystem da EC2
- Os DAGs em `airflow/dags/` vÃªm do cÃ³digo clonado na EC2
- Se o cÃ³digo nÃ£o for atualizado, os DAGs antigos continuam rodando

#### 5.2 Atualizar Imagem Docker na EC2

ApÃ³s atualizar o cÃ³digo, vocÃª precisa atualizar a imagem Docker do ECR:

```bash
# 1. Autenticar no ECR (se ainda nÃ£o estiver autenticado)
export AWS_REGION=us-east-1
export AWS_ACCOUNT_ID=679047180828
aws ecr get-login-password --region $AWS_REGION | \
  docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

# 2. Fazer pull da nova imagem
docker-compose pull

# 3. Reiniciar os serviÃ§os com a nova imagem
docker-compose down
docker-compose up -d

# 4. Verificar se estÃ¡ usando a imagem correta
docker-compose images
```

**Alternativa: Usar `docker-compose.override.yml`**

Para garantir que sempre use a imagem do ECR, crie `docker-compose.override.yml` na EC2:

```bash
# Na EC2, dentro de ~/dataflow-setup
cat > docker-compose.override.yml <<'YAML'
services:
  airflow-init:
    image: 679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:main
    build:  # Remove build local

  airflow-scheduler:
    image: 679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:main

  airflow-webserver:
    image: 679047180828.dkr.ecr.us-east-1.amazonaws.com/dataflow-airflow:main
YAML
```

**Nota**: O `docker-compose.override.yml` Ã© um arquivo local (nÃ£o commitado) que sobrescreve o `docker-compose.yml` na EC2.

#### 5.3 Script de AtualizaÃ§Ã£o AutomÃ¡tica (Opcional)

VocÃª pode criar um script na EC2 para automatizar a atualizaÃ§Ã£o:

```bash
# Na EC2, criar ~/dataflow-setup/update.sh
cat > ~/dataflow-setup/update.sh <<'SCRIPT'
#!/bin/bash
set -e

cd ~/dataflow-setup

echo "ğŸ”„ Atualizando cÃ³digo..."
git pull origin main

echo "ğŸ” Autenticando no ECR..."
export AWS_REGION=us-east-1
export AWS_ACCOUNT_ID=679047180828
aws ecr get-login-password --region $AWS_REGION | \
  docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

echo "ğŸ“¦ Atualizando imagens Docker..."
docker-compose pull

echo "ğŸ”„ Reiniciando serviÃ§os..."
docker-compose down
docker-compose up -d

echo "âœ… AtualizaÃ§Ã£o concluÃ­da!"
docker-compose ps
SCRIPT

chmod +x ~/dataflow-setup/update.sh
```

**Usar o script**:
```bash
~/dataflow-setup/update.sh
```

### 6. **Verificar AtualizaÃ§Ã£o na EC2**

```bash
# 1. Verificar versÃ£o do cÃ³digo
cd ~/dataflow-setup
git log --oneline -1

# 2. Verificar imagem Docker em uso
docker-compose images

# 3. Verificar logs do scheduler (deve mostrar DAGs atualizados)
docker-compose logs airflow-scheduler | tail -20

# 4. Acessar Airflow UI
# http://<IP-EC2>:8080
# Verificar se os DAGs atualizados aparecem
```

### Resumo do Fluxo Completo

| Etapa | Onde | AÃ§Ã£o | AutomÃ¡tico? |
|-------|------|------|-------------|
| 1. Desenvolvimento | Local | Editar cÃ³digo, testar localmente | âŒ Manual |
| 2. PR | GitHub | Criar PR (feature â†’ develop) | âŒ Manual |
| 3. ValidaÃ§Ã£o | GitHub Actions | Lint e validaÃ§Ãµes | âœ… AutomÃ¡tico |
| 4. Merge develop | GitHub | Merge PR para develop | âŒ Manual |
| 5. Build develop | GitHub Actions | Build e push para ECR | âœ… AutomÃ¡tico |
| 6. Merge main | GitHub | Merge develop â†’ main | âŒ Manual |
| 7. Build main | GitHub Actions | Build e push para ECR (tag: main, latest) | âœ… AutomÃ¡tico |
| 8. Atualizar EC2 | EC2 | `git pull` + `docker-compose pull` | âŒ Manual (ou script) |

**Dica**: Para automatizar a etapa 8, vocÃª pode configurar um cron job na EC2 ou usar AWS Systems Manager para executar o script de atualizaÃ§Ã£o periodicamente.

## ğŸ› ï¸ ImplementaÃ§Ã£o Passo a Passo

### Passo 1: Configurar AWS ECR

#### 1.1 Setup Inicial do ECR

```bash
# Configurar variÃ¡veis
export AWS_ACCOUNT_ID=123456789012
export AWS_REGION=us-east-1
export ECR_REPO_NAME=dataflow-airflow

# Criar repositÃ³rio ECR
./airflow/setup-ecr.sh
```

Isso cria um repositÃ³rio ECR com:
- Scanning de vulnerabilidades habilitado
- Criptografia AES256
- Tags mutÃ¡veis

#### 1.2 Build e Push da Imagem

```bash
# Build e push para ECR
./airflow/build-ecr.sh v1.0.0

# Ou usar tag especÃ­fica
./airflow/build-ecr.sh develop
./airflow/build-ecr.sh main
```

**Custos ECR** (para treinamento):
- **Storage**: Primeiros 500MB/mÃªs = **GRATUITO** âœ…
- **Data Transfer**: Primeiro 1GB/mÃªs = **GRATUITO** âœ…
- **Total estimado**: **$0.00/mÃªs** âœ…

Veja `ECR-COSTS-AND-ALTERNATIVES.md` para mais detalhes.

---

### Passo 2: Configurar Airflow Local (Desenvolvimento)

#### 2.1 Docker Compose

O arquivo `docker-compose.yml` jÃ¡ estÃ¡ configurado na raiz do projeto:

```yaml
services:
  postgres:
    image: postgres:16-alpine
    # ... configuraÃ§Ã£o ...

  airflow-init:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    image: dataflow-airflow:latest
    # ... configuraÃ§Ã£o ...

  airflow-scheduler:
    image: dataflow-airflow:latest
    command: ["airflow", "scheduler"]
    # ... configuraÃ§Ã£o ...

  airflow-webserver:
    image: dataflow-airflow:latest
    command: ["airflow", "webserver"]
    ports:
      - "8080:8080"
    # ... configuraÃ§Ã£o ...
```

#### 2.2 Dockerfile

O Dockerfile (`airflow/Dockerfile`) usa:
- **UV** para gerenciar dependÃªncias (mais rÃ¡pido que pip)
- **pyproject.toml** para instalar dependÃªncias (nÃ£o hÃ¡ mais `requirements.txt`)
- Imagem base: `apache/airflow:2.9.3-python3.12`

#### 2.3 Iniciar Ambiente Local

```bash
# Build da imagem local
docker-compose build

# Iniciar todos os serviÃ§os
docker-compose up -d

# Ver logs
docker-compose logs -f airflow-scheduler

# Acessar Airflow UI
open http://localhost:8080
# UsuÃ¡rio: admin
# Senha: admin
```

**Acesso**: `http://localhost:8080` (apenas local)

Veja `docker-compose.md` para explicaÃ§Ã£o detalhada.

---

### Passo 3: Configurar Airflow com ECR (DemonstraÃ§Ã£o/Compartilhamento)

Para ter um **link pÃºblico** que outros usuÃ¡rios podem acessar:

#### 3.1 OpÃ§Ã£o: EC2 com IP PÃºblico ğŸ’° **~$5-10/mÃªs**

**Como funciona**:
- InstÃ¢ncia EC2 roda Docker Compose
- Imagem do ECR Ã© usada no EC2
- IP pÃºblico do EC2 expÃµe porta 8080
- Link pÃºblico disponÃ­vel

**ConfiguraÃ§Ã£o**:

```bash
# 1. Criar instÃ¢ncia EC2 (t2.micro - elegÃ­vel para free tier)
# 2. Conectar via SSH
ssh -i ~/.ssh/your-key.pem ec2-user@<IP-EC2>

# 3. Instalar Docker e Docker Compose
sudo yum update -y
sudo yum install docker -y
sudo service docker start
sudo usermod -a -G docker ec2-user

# Instalar Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# 4. Instalar AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# 5. Configurar credenciais AWS
aws configure

# 6. Autenticar no ECR
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com

# 7. Clone o repositÃ³rio
git clone <your-repo>
cd dataflow-setup

# 8. Criar docker-compose.override.yml para usar imagem do ECR
# (Isso sobrescreve o build local e usa a imagem do ECR)
cat > docker-compose.override.yml <<'YAML'
services:
  airflow-init:
    image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/dataflow-airflow:main

  airflow-scheduler:
    image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/dataflow-airflow:main

  airflow-webserver:
    image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/dataflow-airflow:main
YAML

# 9. Fazer pull da imagem do ECR
docker-compose pull

# 10. Iniciar serviÃ§os
docker-compose up -d

# 10. Configurar Security Group para permitir porta 8080
aws ec2 authorize-security-group-ingress \
  --group-id sg-xxxxxxxxx \
  --protocol tcp \
  --port 8080 \
  --cidr 0.0.0.0/0  # âš ï¸ Apenas para treinamento!

# 11. Acessar
http://<IP-PUBLICO-EC2>:8080
```

**Link especÃ­fico com Elastic IP**:

```bash
# Alocar Elastic IP (IP fixo)
aws ec2 allocate-address --domain vpc

# Associar ao EC2
aws ec2 associate-address \
  --instance-id i-xxxxxxxxx \
  --allocation-id eipalloc-xxxxxxxxx

# Agora vocÃª tem um IP fixo
# Link: http://<ELASTIC-IP>:8080
```

**SeguranÃ§a**:
- âš ï¸ **Configure autenticaÃ§Ã£o do Airflow** (usuÃ¡rio/senha jÃ¡ tem: admin/admin)
- âš ï¸ **Use whitelist de IPs** quando possÃ­vel (nÃ£o permita 0.0.0.0/0 em produÃ§Ã£o)
- âœ… **Use SSH Tunnel** para acesso mais seguro (veja `AIRFLOW-UI-ACCESS.md`)

Veja `AIRFLOW-UI-ACCESS.md` para todas as opÃ§Ãµes de acesso e seguranÃ§a.

---

### Passo 4: Configurar DBT para ProduÃ§Ã£o

#### 4.1 Profiles para ProduÃ§Ã£o

O arquivo `.dbt/profiles.yml` jÃ¡ estÃ¡ configurado na raiz:

```yaml
my_dbt_project:
  target: dev
  outputs:
    defaults: &snowflake_defaults
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      role: "{{ env_var('SNOWFLAKE_ROLE') }}"
      private_key_path: "{{ env_var('SNOWFLAKE_PRIVATE_KEY_PATH') }}"
      private_key_passphrase: "{{ env_var('SNOWFLAKE_PRIVATE_KEY_PASSPHRASE') }}"
      # ...

    dev:
      <<: *snowflake_defaults
      schema: "{{ env_var('SNOWFLAKE_SCHEMA') }}"

    prod:
      <<: *snowflake_defaults
      database: "{{ env_var('SNOWFLAKE_DATABASE_PROD') }}"
      schema: prod
```

#### 4.2 VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz:

```bash
# Snowflake
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_ROLE=your_role
SNOWFLAKE_DATABASE_DEV=your_dev_database
SNOWFLAKE_DATABASE_PROD=your_prod_database
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_SCHEMA=your_schema
SNOWFLAKE_PRIVATE_KEY_PATH=/opt/airflow/.dbt/rsa_key.p8
SNOWFLAKE_PRIVATE_KEY_PASSPHRASE=your_passphrase
SNOWFLAKE_QUERY_TAG=airflow_production

# AWS
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=123456789012
```

---

### Passo 5: Configurar GitHub Actions (CI/CD)

#### 5.1 Secrets no GitHub

No seu repositÃ³rio GitHub, vÃ¡ para **Settings > Secrets and variables > Actions** e adicione:

**Secrets obrigatÃ³rios para build ECR**:
```
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_ACCOUNT_ID=123456789012
AWS_REGION=us-east-1
```

**Secrets opcionais (para outros workflows)**:
```
SNOWFLAKE_ACCOUNT=your-account
SNOWFLAKE_USER=your-user
SNOWFLAKE_ROLE=your-role
SNOWFLAKE_DATABASE_DEV=your-dev-database
SNOWFLAKE_DATABASE_PROD=your-prod-database
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_SCHEMA=your-schema
```

**Como adicionar secrets**:
1. VÃ¡ em **Settings** do repositÃ³rio
2. **Secrets and variables** â†’ **Actions**
3. Clique em **New repository secret**
4. Adicione cada secret acima

#### 5.2 Workflows Configurados

O projeto jÃ¡ possui dois workflows configurados:

**1. `.github/workflows/lint_on_push.yml`** (ValidaÃ§Ã£o):
- âœ… Executa em PRs e pushes para `main`/`develop`
- âœ… Valida cÃ³digo com pre-commit hooks
- âœ… Lint de Python (black, isort, flake8)
- âœ… Lint de SQL (sqlfluff)
- âœ… Valida apenas arquivos modificados

**2. `.github/workflows/build-and-push-ecr.yml`** (Build e Deploy):
- âœ… Executa em push para `main` ou `develop`
- âœ… Build da imagem Docker usando `airflow/Dockerfile`
- âœ… Push para ECR com tags:
  - `develop` â†’ tag: `develop` + `<SHA>`
  - `main` â†’ tag: `main` + `latest` + `<SHA>`

**Como funciona o workflow de build**:

1. **Trigger**: Push para `main` ou `develop`
2. **Build**: Usa `airflow/Dockerfile` com contexto na raiz do projeto
3. **Tags**:
   - Branch `develop` â†’ `dataflow-airflow:develop` + `dataflow-airflow:<SHA>`
   - Branch `main` â†’ `dataflow-airflow:main` + `dataflow-airflow:latest` + `dataflow-airflow:<SHA>`
4. **Push**: Envia todas as tags para o ECR

**Verificar execuÃ§Ã£o**:
- VÃ¡ em **Actions** no GitHub
- Veja o workflow "Build and Push to ECR"
- Clique na execuÃ§Ã£o para ver logs detalhados

**ExecuÃ§Ã£o manual**:
- O workflow tambÃ©m pode ser executado manualmente via **Actions** â†’ **Build and Push to ECR** â†’ **Run workflow**

---

## ğŸš€ Executando o Pipeline

### 1. **Setup Inicial (Local)**

```bash
# Clone o repositÃ³rio
git clone <your-repo>
cd dataflow-setup

# Configure variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas credenciais

# Inicie o Airflow local
docker-compose up -d

# Acesse o Airflow UI
open http://localhost:8080
# UsuÃ¡rio: admin
# Senha: admin
```

### 2. **ExecuÃ§Ã£o Manual**

```bash
# Execute DBT localmente
dbt build --target dev

# Ou via Airflow UI
# 1. Acesse http://localhost:8080
# 2. Encontre seu DAG
# 3. Clique em "Trigger DAG"
```

### 3. **ExecuÃ§Ã£o AutomÃ¡tica**

Os DAGs executam automaticamente conforme o schedule definido nos DAGs.

---

## ğŸ“Š Monitoramento e Logs

### 1. **Logs do Airflow**
- **Web UI**: http://localhost:8080 (local) ou http://<IP-EC2>:8080 (EC2)
- **Logs locais**: `./logs/airflow/`
- **S3**: Logs podem ser enviados para S3 (configurar no Airflow)

### 2. **Logs do DBT**
- **Local**: `./logs/dbt/`
- **S3**: Configurar para enviar logs para S3

### 3. **MÃ©tricas**
- **Airflow**: MÃ©tricas disponÃ­veis no UI
- **DBT**: Logs detalhados de execuÃ§Ã£o
- **Snowflake**: Query history e performance

---

## ğŸ”§ Troubleshooting

### 1. **DAGs nÃ£o aparecem no Airflow**
```bash
# Verificar logs do scheduler
docker-compose logs airflow-scheduler

# Verificar sintaxe dos DAGs
python -m py_compile airflow/dags/*.py
```

### 2. **DBT nÃ£o conecta no Snowflake**
```bash
# Testar conexÃ£o
dbt debug --target dev

# Verificar variÃ¡veis de ambiente
echo $SNOWFLAKE_ACCOUNT
```

### 3. **Erro ao autenticar no ECR**
```bash
# Autenticar manualmente
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com

# Verificar credenciais
aws sts get-caller-identity
```

### 4. **Imagem do ECR nÃ£o encontra pyproject.toml**
```bash
# Verificar se o build context estÃ¡ correto
# No docker-compose.yml, context deve ser "." (raiz do projeto)
# E o Dockerfile deve estar em airflow/Dockerfile
```

### 5. **CÃ³digo na EC2 nÃ£o estÃ¡ atualizado**
```bash
# Na EC2, verificar branch e atualizar cÃ³digo
cd ~/dataflow-setup
git branch  # Deve estar em main ou develop
git pull origin main  # Atualizar cÃ³digo

# Verificar se os DAGs foram atualizados
ls -la airflow/dags/

# Reiniciar serviÃ§os para carregar novos DAGs
docker-compose restart airflow-scheduler
```

### 6. **Imagem na EC2 nÃ£o estÃ¡ atualizada**
```bash
# Na EC2, autenticar no ECR e fazer pull da nova imagem
export AWS_REGION=us-east-1
export AWS_ACCOUNT_ID=679047180828
aws ecr get-login-password --region $AWS_REGION | \
  docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

# Fazer pull da nova imagem
docker-compose pull

# Reiniciar serviÃ§os
docker-compose down
docker-compose up -d

# Verificar qual imagem estÃ¡ sendo usada
docker-compose images
```

---

## ğŸ’° Estimativa de Custos

### **Desenvolvimento Local (Docker Compose):**
- **Custo**: **$0.00** âœ…
- **Acesso**: http://localhost:8080 (apenas local)

### **DemonstraÃ§Ã£o/Compartilhamento (EC2):**
- **EC2 t2.micro**: **$0.00** (free tier) ou **~$7-10/mÃªs** âœ…
- **ECR**: **$0.00** (free tier cobre) âœ…
- **S3**: **$0.00** (free tier cobre) âœ…
- **Total**: **~$0-10/mÃªs** âœ…
- **Acesso**: http://<IP-EC2>:8080 (pÃºblico)

### **ProduÃ§Ã£o EscalÃ¡vel (ECS/EKS):**
- **ECS/EKS**: ~$50-200/mÃªs (nÃ£o recomendado para treinamento)
- **ALB**: ~$20/mÃªs adicional

---

## ğŸ“š DocumentaÃ§Ã£o Relacionada

- **`docker-compose.md`**: ExplicaÃ§Ã£o detalhada do docker-compose.yml
- **`airflow/README.md`**: DocumentaÃ§Ã£o completa da infraestrutura Airflow e ECR
- **`ECR-COSTS-AND-ALTERNATIVES.md`**: Custos e alternativas ao ECR
- **`AIRFLOW-UI-ACCESS.md`**: Todas as opÃ§Ãµes de acesso Ã  UI do Airflow

---

## ğŸ¯ PrÃ³ximos Passos

1. âœ… **Configurar** AWS ECR (praticamente gratuito)
2. âœ… **Testar** pipeline completo localmente
3. âœ… **Deploy** para EC2 (se quiser link pÃºblico)
4. âœ… **Configurar** CI/CD com GitHub Actions
5. âœ… **Documentar** processos especÃ­ficos do seu time

---

**Nota**: Este guia foca em uma stack completa para treinamento com custos baixos (~$0-10/mÃªs). ECR Ã© praticamente gratuito no free tier, e EC2 pode ser gratuito (free tier) ou muito barato (~$7-10/mÃªs). Para produÃ§Ã£o real com alta disponibilidade, considere ECS/EKS (custo mais alto).
