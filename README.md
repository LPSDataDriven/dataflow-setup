## Base de setup para projeto de dados com dbt, Snowflake, Airflow e AWS

Este repositÃ³rio Ã© um template de partida para criar um projeto do zero contemplando:

- dbt Core (modelagem, testes, documentaÃ§Ã£o)
- Snowflake (warehouse, Snowpark, `snowflake-connector-python`)
- Airflow (orquestraÃ§Ã£o local com Docker)
- AWS (S3 para artefatos/logs, Secrets Manager, IAM)

O objetivo Ã© fornecer um guia de setup inicial robusto e reaproveitÃ¡vel, alÃ©m de uma estrutura base para evoluÃ§Ã£o do projeto.

### SumÃ¡rio

- VisÃ£o geral e prÃ©â€‘requisitos
- Setup de ambiente com `uv` e `direnv`
- ExplicaÃ§Ã£o do `pyproject.toml`
- Estrutura sugerida do repositÃ³rio
- ConfiguraÃ§Ã£o do dbt + Snowflake (`profiles.yml`, chave RSA, conexÃ£o)
- Qualidade de cÃ³digo: `pre-commit` e `sqlfluff`
- IntegraÃ§Ã£o com Airflow e AWS (local, ECR e EC2)
- Comandos Ãºteis
- Troubleshooting

## VisÃ£o geral e prÃ©â€‘requisitos

PrÃ©â€‘requisitos recomendados:

- Python 3.12+
- Git
- [uv](https://docs.astral.sh/uv/) para gerenciar virtualenv e dependÃªncias
- [direnv](https://direnv.net/) para carregar `.env` e ativar o venv automaticamente
- Acesso a uma conta Snowflake com role para criaÃ§Ã£o/leitura de objetos
- Conta AWS (opcional nesta fase) com acesso a S3 e Secrets Manager

## Setup de ambiente (uv + direnv)

1) Crie/clonar o repositÃ³rio
```bash
git clone git@github.com:<accountName>/<repoName>.git
cd <repoName>
```

2) Instale o `uv` (se necessÃ¡rio)
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```
Outras opÃ§Ãµes: consulte [instalaÃ§Ã£o do uv](https://docs.astral.sh/uv/getting-started/installation/).

3) Crie o ambiente virtual
```bash
uv venv --python=3.12
```
Isso criarÃ¡ a pasta `.venv` na raiz do projeto.

4) Ative o ambiente virtual (manual, se nÃ£o usar direnv)
```bash
source .venv/bin/activate
```

5) VariÃ¡veis de ambiente e `direnv`
- Crie um arquivo `.env` na raiz (baseado em `.env.example` se existir) com as variÃ¡veis sensÃ­veis (Snowflake, AWS, etc.).
- Crie um `.envrc` com o conteÃºdo abaixo:
```bash
dotenv
if [ -f .venv/bin/activate ]; then
    source .venv/bin/activate
fi
unset PS1
```
Instale o `direnv` e habilite no shell:
```bash
# Ubuntu/Debian
sudo apt-get install direnv

# bash
echo 'eval "$(direnv hook bash)"' >> ~/.bashrc
# zsh
echo 'eval "$(direnv hook zsh)"' >> ~/.zshrc

direnv allow
```
ObservaÃ§Ã£o: o aviso "PS1 cannot be exported" pode aparecer e Ã© esperado; nÃ£o impacta o funcionamento.

## `pyproject.toml`: dependÃªncias e tooling

Este arquivo centraliza dependÃªncias e ferramentas do projeto.

### Estrutura das dependÃªncias

**`[project].dependencies`** - DependÃªncias de produÃ§Ã£o (sempre instaladas):
  - `dbt-core`, `dbt-snowflake`: base do dbt e adaptador Snowflake
  - `pre-commit`, `sqlfluff`: qualidade e lint de SQL
  - `recce`: ferramenta de QA para dados
  - `pandas`: utilidades de manipulaÃ§Ã£o de dados
  - `snowflake-snowpark-python`, `snowflake-connector-python[pandas]`: integraÃ§Ã£o com Snowflake
  - `cryptography`: suporte a chaves RSA

**`[tool.uv].dev-dependencies`** - DependÃªncias de desenvolvimento (apenas para desenvolvimento local):
  - `pytest`, `pytest-mock`, `pytest-cov`, `moto`: testes
  - `black`, `isort`, `flake8`: formatadores e linters
  - `apache-airflow>=2.8.0`: Airflow para desenvolvimento local
  - `awscli`, `aws-sam-cli`: ferramentas AWS para desenvolvimento

### InstalaÃ§Ã£o das dependÃªncias

**Para desenvolvimento local** (recomendado):
```bash
uv pip install -e ".[dev]"
```
Instala **todas** as dependÃªncias: produÃ§Ã£o + desenvolvimento (pytest, black, isort, etc.)

**Para produÃ§Ã£o** (apenas dependÃªncias essenciais):
```bash
uv pip install -e .
```
Instala **apenas** as dependÃªncias de produÃ§Ã£o (sem black, isort, pytest, etc.)

### ConfiguraÃ§Ãµes de formataÃ§Ã£o

- `[tool.black]`: configuraÃ§Ã£o do Black (formataÃ§Ã£o automÃ¡tica)
- `[tool.isort]`: configuraÃ§Ã£o do isort (ordenaÃ§Ã£o de imports)

## Estrutura sugerida do repositÃ³rio

Exemplo mÃ­nimo (ajuste conforme a necessidade):
```
.
â”œâ”€â”€ .dbt/
â”‚   â””â”€â”€ profiles.yml               # perfis do dbt (NÃƒO commitar credenciais reais)
â”œâ”€â”€ dbt/                           # projeto dbt
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/                    # modelos dbt (staging, marts, etc.)
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ analyses/
â”‚   â”œâ”€â”€ macros/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ dataflow/                      # utilitÃ¡rios Python para Snowflake/Snowpark
â”œâ”€â”€ airflow/                       # configuraÃ§Ã£o do Airflow (Docker, ECR)
â”‚   â”œâ”€â”€ Dockerfile                 # Imagem Docker usando UV
â”‚   â”œâ”€â”€ build-ecr.sh              # Script para build/push ECR
â”‚   â”œâ”€â”€ setup-ecr.sh              # Script para criar repositÃ³rio ECR
â”‚   â””â”€â”€ dags/                      # DAGs do Airflow (ex.: execuÃ§Ã£o de dbt)
â”œâ”€â”€ .sqlfluff
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ .envrc
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## ConfiguraÃ§Ã£o dbt + Snowflake

1) Crie/configure `~/.dbt/profiles.yml` OU use uma pasta `.dbt/` no repositÃ³rio com `profiles.yml` (como neste template). Exemplo de autenticaÃ§Ã£o via chave RSA usando variÃ¡veis de ambiente:
```yaml
my_dbt_project:
  target: dev
  outputs:
    # Common Snowflake connection settings
    defaults: &snowflake_defaults
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      role: "{{ env_var('SNOWFLAKE_ROLE') }}"
      private_key_path: "{{ env_var('SNOWFLAKE_PRIVATE_KEY_PATH') }}"
      private_key_passphrase: "{{ env_var('SNOWFLAKE_PRIVATE_KEY_PASSPHRASE') }}"
      database: "{{ env_var('SNOWFLAKE_DATABASE_DEV') }}"
      warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
      client_session_keep_alive: False
      threads: 10
      query_tag: "{{ env_var('SNOWFLAKE_QUERY_TAG') }}"

    dev:
      <<: *snowflake_defaults
      schema: "{{ env_var('SNOWFLAKE_SCHEMA') }}"

    stage:
      <<: *snowflake_defaults
      schema: stage

    # For read-only access to prod for recce
    prod-read-only:
      <<: *snowflake_defaults
      database: "{{ env_var('SNOWFLAKE_DATABASE_PROD') }}"
      schema: prod
```

3) Chave RSA no Snowflake
- Gere um par de chaves RSA localmente e configure a chave pÃºblica no usuÃ¡rio Snowflake.
- Armazene o caminho da chave privada em `${SNOWFLAKE_PRIVATE_KEY_PATH}` no `.env`.
- Se a chave privada estiver protegida por senha (recomendado), configure tambÃ©m `${SNOWFLAKE_PRIVATE_KEY_PASSPHRASE}` no `.env` com a senha da chave.

4) Verifique a instalaÃ§Ã£o e a conexÃ£o
```bash
dbt --version
dbt debug
dbt build
```

## Qualidade de cÃ³digo: pre-commit e sqlfluff

O [pre-commit](https://pre-commit.com/) Ã© um framework para gerenciar hooks de git que executam verificaÃ§Ãµes antes de cada commit. Ele permite identificar problemas simples (como trailing whitespace, formataÃ§Ã£o incorreta, problemas de lint) antes da revisÃ£o de cÃ³digo, economizando tempo tanto do desenvolvedor quanto do revisor.

1) Garanta que os arquivos `.pre-commit-config.yaml` e `.sqlfluff` existam e reflitam suas regras.

2) Verifique a instalaÃ§Ã£o e configure os hooks do git
```bash
# Verifique se o pre-commit estÃ¡ instalado (jÃ¡ vem com uv pip install -e .)
pre-commit --version

# Instale os hooks do git (isso configura os scripts em .git/hooks/pre-commit)
pre-commit install
```

ApÃ³s isso, os hooks rodam automaticamente em cada `git commit`. O pre-commit gerencia o download e execuÃ§Ã£o de qualquer hook escrito em qualquer linguagem antes de cada commit.

3) (Opcional) Execute os hooks em todos os arquivos
Quando vocÃª adiciona novos hooks, Ã© recomendado executÃ¡-los em todos os arquivos do projeto para garantir que tudo estÃ¡ em conformidade:
```bash
pre-commit run --all-files
```

Isso executa todos os hooks configurados em todos os arquivos, nÃ£o apenas nos arquivos alterados. Durante um commit normal, o pre-commit executa apenas nos arquivos modificados.

4) Executar hooks manualmente
VocÃª tambÃ©m pode executar os hooks manualmente sem fazer commit:
```bash
# Executa apenas nos arquivos staged
pre-commit run

# Executa um hook especÃ­fico
pre-commit run sqlfluff-lint --all-files
```

## IntegraÃ§Ã£o com Airflow e AWS

Esta base nÃ£o impÃµe uma stack especÃ­fica, mas sugere caminhos:

### Airflow Local (Desenvolvimento)

Para desenvolvimento local, utilize o `docker-compose.yml` com `webserver`, `scheduler` e `postgres`:

```bash
# Build da imagem local
docker-compose build

# Iniciar todos os serviÃ§os
docker-compose up -d

# Acessar Airflow UI
open http://localhost:8080
# UsuÃ¡rio: admin
# Senha: admin
```

Uma DAG tÃ­pica chama o dbt via CLI (por exemplo, `dbt build`). Veja [docs/DOCKER-COMPOSE.md](docs/DOCKER-COMPOSE.md) para explicaÃ§Ã£o detalhada.

### AWS ECR (Registry de Imagens Docker)

O **AWS ECR** Ã© usado para armazenar a imagem Docker do Airflow, permitindo compartilhar e usar a mesma imagem em diferentes ambientes (local, EC2, etc.).

**Setup inicial do ECR:**

```bash
# Configurar variÃ¡veis
export AWS_ACCOUNT_ID=<seu-account-id>
export AWS_REGION=us-east-1
export ECR_REPO_NAME=dataflow-airflow

# Criar repositÃ³rio ECR
./airflow/setup-ecr.sh
```

**Build e push da imagem:**

```bash
# Build e push para ECR
./airflow/build-ecr.sh develop  # ou main, v1.0.0, etc.
```

**Custos ECR** (free tier):
- Storage: Primeiros 500MB/mÃªs = **GRATUITO** âœ…
- Data Transfer: Primeiro 1GB/mÃªs = **GRATUITO** âœ…
- Total estimado: **$0.00/mÃªs** para projetos pequenos/mÃ©dios âœ…

ðŸ“š **DocumentaÃ§Ã£o completa**: Veja [docs/ECR-COSTS-AND-ALTERNATIVES.md](docs/ECR-COSTS-AND-ALTERNATIVES.md) para detalhes de custos e alternativas.

### AWS EC2 (ProduÃ§Ã£o/DemonstraÃ§Ã£o)

Para ter um ambiente acessÃ­vel publicamente ou para produÃ§Ã£o, vocÃª pode rodar o Airflow em uma instÃ¢ncia EC2 usando a imagem do ECR.

**Passos principais:**

1. **Criar instÃ¢ncia EC2** (t3.small recomendado, t3.micro funciona)
   - AMI: Amazon Linux 2023
   - Security Group: SSH (22) + Airflow UI (8080)
   - IAM Role com polÃ­tica `AmazonEC2ContainerRegistryReadOnly` (opcional, facilita acesso ao ECR)

2. **Instalar Docker e Docker Compose na EC2:**
```bash
sudo dnf update -y
sudo dnf install -y docker git awscli
sudo systemctl enable --now docker
sudo usermod -aG docker ec2-user

# Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

3. **Autenticar no ECR e clonar repositÃ³rio:**
```bash
# Autenticar no ECR
export AWS_REGION=us-east-1
export AWS_ACCOUNT_ID=<seu-account-id>
aws ecr get-login-password --region $AWS_REGION | \
  docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com

# Clonar repositÃ³rio
git clone <seu-repo> dataflow-setup
cd dataflow-setup
```

4. **Configurar docker-compose.override.yml para usar imagem do ECR:**
```bash
cat > docker-compose.override.yml <<'YAML'
services:
  airflow-init:
    image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/dataflow-airflow:main

  airflow-scheduler:
    image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/dataflow-airflow:main

  airflow-webserver:
    image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/dataflow-airflow:main
YAML
```

5. **Iniciar serviÃ§os:**
```bash
docker-compose pull
docker-compose up -d
```

**Custos EC2** (estimativa):
- t3.micro: **$0.00** (free tier) ou **~$7-10/mÃªs** apÃ³s free tier
- t3.small: **~$15-20/mÃªs**
- Elastic IP: **$0.00** se associado a instÃ¢ncia rodando

ðŸ“š **DocumentaÃ§Ã£o completa**: Veja [docs/EC2-AIRFLOW-ECR-SETUP.md](docs/EC2-AIRFLOW-ECR-SETUP.md) para guia passo a passo detalhado.

### CI/CD com GitHub Actions

O projeto inclui workflows do GitHub Actions para:
- **ValidaÃ§Ã£o automÃ¡tica** (lint, pre-commit) em PRs
- **Build e push automÃ¡tico** para ECR ao fazer merge em `develop` ou `main`

ðŸ“š **DocumentaÃ§Ã£o completa**: Veja [docs/PRODUCTION-GUIDE.md](docs/PRODUCTION-GUIDE.md) para fluxo completo de CI/CD e produÃ§Ã£o.

### Outros ServiÃ§os AWS

- **AWS S3**: armazene artefatos de execuÃ§Ã£o (logs, manifest.json, run_results.json) e seeds estÃ¡ticos.
- **AWS Secrets Manager**: gerencie credenciais (Snowflake, etc.) e injete-as via conexÃ£o/variÃ¡veis do Airflow.

### Pontos de atenÃ§Ã£o

- ConexÃ£o do Airflow com Snowflake deve usar a mesma modalidade de autenticaÃ§Ã£o (idealmente chave RSA ou usuÃ¡rio/senha rotacionada via Secrets Manager).
- Garanta que as roles Snowflake e permissÃµes IAM (S3/Secrets/ECR) estejam corretas.
- Para acesso seguro Ã  UI do Airflow em EC2, considere usar SSH Tunnel (veja [docs/AIRFLOW-UI-ACCESS.md](docs/AIRFLOW-UI-ACCESS.md)).

## Comandos Ãºteis

Ambiente e dependÃªncias:
```bash
# Criar ambiente virtual
uv venv --python=3.12

# Para desenvolvimento (instala tudo: produÃ§Ã£o + dev)
uv pip install -e ".[dev]"

# Para produÃ§Ã£o (apenas dependÃªncias essenciais)
uv pip install -e .
```

dbt bÃ¡sico:
```bash
dbt debug
dbt deps
dbt seed
dbt run
dbt test
dbt build
dbt docs generate && dbt docs serve
```

Formatadores e lint:
```bash
black .
isort .
flake8
sqlfluff lint
sqlfluff fix
```

Snowflake (Python):
```bash
python -c "from snowflake import connector; print('ok')"
```

## Troubleshooting

- dbt nÃ£o conecta ao Snowflake: confira `profiles.yml`, variÃ¡veis do `.env`, role/warehouse/schema, e se a chave RSA estÃ¡ correta.
- `direnv` nÃ£o carrega: confirme `direnv allow` e o hook no shell.
- Falha em hooks `pre-commit`: rode os linters localmente para corrigir (`black`, `isort`, `sqlfluff`).
- Incompatibilidade de versÃ£o: valide Python 3.12+, versÃµes do `dbt-snowflake` e do conector no `pyproject.toml`.

---

Este README serve como base inicial e pode ser estendido conforme o projeto evolui (monitoramento, CI/CD, testes de dados avanÃ§ados, etc.).
