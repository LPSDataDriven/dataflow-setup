# üöÄ Guia Completo de Implementa√ß√£o em Produ√ß√£o

## GitHub + Airflow Local + DBT Core

Este guia te mostra como implementar um pipeline de dados completo em produ√ß√£o usando GitHub para versionamento, Airflow local com Docker para orquestra√ß√£o e DBT Core para transforma√ß√µes.

## üìã Vis√£o Geral da Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   GitHub Repo   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   GitHub Actions‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   S3 Bucket     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ   (CI/CD)       ‚îÇ    ‚îÇ   (Artifacts)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                       ‚îÇ
                                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Snowflake     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   DBT Core      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Airflow Local ‚îÇ
‚îÇ   (Data Lake)   ‚îÇ    ‚îÇ   (Transform)   ‚îÇ    ‚îÇ   (Docker)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîÑ Fluxo Completo de Produ√ß√£o

### 1. **Desenvolvimento Local**
```bash
# Desenvolver DAGs e modelos DBT
git checkout -b feature/new-pipeline
# ... fazer mudan√ßas ...
git add .
git commit -m "Add new DBT pipeline"
git push origin feature/new-pipeline
```

### 2. **Pull Request**
- GitHub Actions executa valida√ß√µes
- Testa DAGs e modelos DBT
- Valida sintaxe e conectividade

### 3. **Merge para Main**
- GitHub Actions faz deploy autom√°tico
- Upload para S3 bucket (artifacts)
- Airflow local detecta novos DAGs

### 4. **Execu√ß√£o no Airflow Local**
- Scheduler executa DAGs conforme cron
- Tasks executam comandos DBT
- DBT conecta no Snowflake e executa SQL
- Logs s√£o salvos localmente e no S3

## üõ†Ô∏è Implementa√ß√£o Passo a Passo

### Passo 1: Configurar GitHub Actions

#### 1.1 Criar Secrets no GitHub
No seu reposit√≥rio GitHub, v√° para **Settings > Secrets and variables > Actions** e adicione:

```
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
SNOWFLAKE_ACCOUNT=your-account
SNOWFLAKE_USER=your-user
SNOWFLAKE_PASSWORD=your-password
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_DATABASE_DEV=your-dev-database
SNOWFLAKE_DATABASE_PROD=your-prod-database
SNOWFLAKE_SCHEMA=your-schema
SNOWFLAKE_ROLE=your-role
```

#### 1.2 Configurar Workflow
O arquivo `.github/workflows/test-cicd.yml` j√° foi criado e inclui:
- **Valida√ß√£o**: Testa secrets e conectividade
- **Testes**: Valida DAGs e modelos DBT
- **Deploy**: Upload para S3

### Passo 2: Configurar Airflow Local

#### 2.1 Docker Compose
Crie um `docker-compose.yml` na raiz do projeto:

```yaml
version: '3.8'
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  airflow-webserver:
    build: ./airflow-local
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      postgres:
        condition: service_healthy

  airflow-scheduler:
    build: ./airflow-local
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  postgres_db_volume:
```

#### 2.2 Dockerfile para Airflow
Crie `airflow-local/Dockerfile`:

```dockerfile
FROM apache/airflow:2.8.1-python3.11

USER root
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

USER airflow
COPY requirements/requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

COPY dags/ /opt/airflow/dags/
COPY .dbt/ /opt/airflow/.dbt/
COPY dbt/ /opt/airflow/dbt/
```

### Passo 3: Configurar DBT para Produ√ß√£o

#### 3.1 Profiles para Produ√ß√£o
Atualize `.dbt/profiles.yml`:

```yaml
dataflow_setup:
  target: prod
  outputs:
    dev:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      role: "{{ env_var('SNOWFLAKE_ROLE') }}"
      database: "{{ env_var('SNOWFLAKE_DATABASE_DEV') }}"
      warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
      schema: "{{ env_var('SNOWFLAKE_SCHEMA') }}"
      authenticator: password
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      client_session_keep_alive: true
      query_tag: "dbt_local_development"

    prod:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      role: "{{ env_var('SNOWFLAKE_ROLE') }}"
      database: "{{ env_var('SNOWFLAKE_DATABASE_PROD') }}"
      warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
      schema: "{{ env_var('SNOWFLAKE_SCHEMA') }}"
      authenticator: password
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      client_session_keep_alive: true
      query_tag: "dbt_local_production"
```

### Passo 4: Configurar Monitoramento

#### 4.1 CloudWatch Alarms (Opcional)
Para monitorar execu√ß√µes via S3:

```bash
aws cloudwatch put-metric-alarm \
  --alarm-name "DBT-Pipeline-Failures" \
  --alarm-description "Alert when DBT pipeline fails" \
  --metric-name "FailedExecutions" \
  --namespace "Custom/DBT" \
  --statistic "Sum" \
  --period 300 \
  --threshold 1 \
  --comparison-operator "GreaterThanOrEqualToThreshold" \
  --evaluation-periods 1
```

#### 4.2 Notifica√ß√µes SNS
```bash
aws sns create-topic --name "dbt-alerts"

aws sns subscribe \
  --topic-arn "arn:aws:sns:us-east-1:123456789012:dbt-alerts" \
  --protocol "email" \
  --notification-endpoint "your-email@example.com"
```

## üöÄ Executando o Pipeline

### 1. **Setup Inicial**
```bash
# Clone o reposit√≥rio
git clone <your-repo>
cd dataflow-setup

# Configure vari√°veis de ambiente
cp .env.example .env
# Edite .env com suas credenciais

# Inicie o Airflow
docker-compose up -d

# Acesse o Airflow UI
open http://localhost:8080
```

### 2. **Execu√ß√£o Manual**
```bash
# Execute DBT localmente
dbt build --target prod

# Ou via Airflow UI
# 1. Acesse http://localhost:8080
# 2. Encontre seu DAG
# 3. Clique em "Trigger DAG"
```

### 3. **Execu√ß√£o Autom√°tica**
Os DAGs executam automaticamente conforme o schedule definido.

## üìä Monitoramento e Logs

### 1. **Logs do Airflow**
- **Web UI**: http://localhost:8080
- **Logs locais**: `./logs/airflow/`
- **S3**: Logs s√£o enviados para S3 automaticamente

### 2. **Logs do DBT**
- **Local**: `./logs/dbt/`
- **S3**: `s3://your-bucket/dbt-logs/`

### 3. **M√©tricas**
- **Airflow**: M√©tricas dispon√≠veis no UI
- **DBT**: Logs detalhados de execu√ß√£o
- **Snowflake**: Query history e performance

## üîß Troubleshooting

### 1. **DAGs n√£o aparecem no Airflow**
```bash
# Verificar logs do scheduler
docker-compose logs airflow-scheduler

# Verificar sintaxe dos DAGs
python -m py_compile airflow-local/dags/*.py
```

### 2. **DBT n√£o conecta no Snowflake**
```bash
# Testar conex√£o
dbt debug --target prod

# Verificar vari√°veis de ambiente
echo $SNOWFLAKE_ACCOUNT
```

### 3. **Erro de permiss√µes AWS**
```bash
# Verificar credenciais
aws sts get-caller-identity

# Testar acesso ao S3
aws s3 ls s3://your-bucket
```

## üí∞ Estimativa de Custos

### **Execu√ß√£o Local (Docker):**
- **Custo**: $0 (apenas recursos locais)
- **Vantagens**: Controle total, sem custos de cloud
- **Desvantagens**: Requer m√°quina sempre ligada

### **Para Produ√ß√£o Escal√°vel:**
- **AWS ECS/EKS**: ~$50-200/m√™s (dependendo do uso)
- **Google Cloud Run**: ~$30-100/m√™s
- **Azure Container Instances**: ~$40-150/m√™s

## üéØ Pr√≥ximos Passos

1. **Configurar** Airflow local com Docker
2. **Testar** pipeline completo
3. **Implementar** monitoramento
4. **Documentar** processos
5. **Treinar** equipe

---

**Nota**: Este guia foca em execu√ß√£o local para reduzir custos. Para ambientes de produ√ß√£o com alta disponibilidade, considere migrar para solu√ß√µes gerenciadas como ECS, EKS ou outras plataformas de container.
