# üöÄ Guia Completo de Implementa√ß√£o em Produ√ß√£o

## GitHub + AWS MWAA + DBT Core

Este guia te mostra como implementar um pipeline de dados completo em produ√ß√£o usando GitHub para versionamento, AWS MWAA para orquestra√ß√£o e DBT Core para transforma√ß√µes.

## üìã Vis√£o Geral da Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   GitHub Repo   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   GitHub Actions‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   S3 Bucket     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ   (CI/CD)       ‚îÇ    ‚îÇ   (MWAA)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                       ‚îÇ
                                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Snowflake     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   DBT Core      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   AWS MWAA      ‚îÇ
‚îÇ   (Data Lake)   ‚îÇ    ‚îÇ   (Transform)   ‚îÇ    ‚îÇ   (Orchestrate) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîÑ Fluxo Completo de Produ√ß√£o

### 1. **Desenvolvimento Local**
```bash
# Desenvolver DAGs e modelos DBT
git checkout -b feature/new-pipeline
# ... fazer mudan√ßas ...
git add .
git commit -m "Add new DBT pipeline"
git push origin feature/new-pipeline
```

### 2. **Pull Request**
- GitHub Actions executa valida√ß√µes
- Testa DAGs e modelos DBT
- Valida sintaxe e conectividade

### 3. **Merge para Main**
- GitHub Actions faz deploy autom√°tico
- Upload para S3 bucket do MWAA
- MWAA detecta novos DAGs

### 4. **Execu√ß√£o no MWAA**
- Scheduler executa DAGs conforme cron
- Tasks executam comandos DBT
- DBT conecta no Snowflake e executa SQL
- Logs s√£o salvos no CloudWatch

## üõ†Ô∏è Implementa√ß√£o Passo a Passo

### Passo 1: Configurar GitHub Repository

#### 1.1 Estrutura do Reposit√≥rio
```
your-data-pipeline/
‚îú‚îÄ‚îÄ .github/workflows/          # CI/CD pipelines
‚îÇ   ‚îî‚îÄ‚îÄ deploy-mwaa.yml
‚îú‚îÄ‚îÄ airflow-mwaa/              # C√≥digo do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ dags/                  # DAGs
‚îÇ   ‚îú‚îÄ‚îÄ requirements/          # Depend√™ncias Python
‚îÇ   ‚îî‚îÄ‚îÄ plugins/               # Plugins customizados
‚îú‚îÄ‚îÄ my_dbt_project/            # Projeto DBT
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Modelos DBT
‚îÇ   ‚îú‚îÄ‚îÄ tests/                 # Testes DBT
‚îÇ   ‚îú‚îÄ‚îÄ macros/                # Macros DBT
‚îÇ   ‚îî‚îÄ‚îÄ dbt_project.yml        # Configura√ß√£o DBT
‚îú‚îÄ‚îÄ .dbt/                      # Profiles DBT
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml           # Configura√ß√£o de conex√µes
‚îú‚îÄ‚îÄ scripts/                   # Scripts utilit√°rios
‚îî‚îÄ‚îÄ docs/                      # Documenta√ß√£o
```

#### 1.2 Configurar Secrets no GitHub
V√° em **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions** e adicione:

```bash
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
SNOWFLAKE_ACCOUNT=your_account.snowflakecomputing.com
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
```

### Passo 2: Configurar AWS MWAA

#### 2.1 Criar S3 Bucket
```bash
# Criar bucket
aws s3 mb s3://lpsdata-airflow-1

# Criar estrutura de diret√≥rios
aws s3api put-object --bucket lpsdata-airflow-1 --key dags/
aws s3api put-object --bucket lpsdata-airflow-1 --key plugins/
aws s3api put-object --bucket lpsdata-airflow-1 --key requirements/
aws s3api put-object --bucket lpsdata-airflow-1 --key dbt_project/
aws s3api put-object --bucket lpsdata-airflow-1 --key .dbt/
```

#### 2.2 Configurar MWAA Environment
No console AWS MWAA:

1. **Environment Name**: `lpsdata-mwaa-prod`
2. **Airflow Version**: `2.8.1`
3. **Python Version**: `3.11`
4. **Requirements File**: `requirements/requirements.txt`
5. **DAGs Folder**: `dags/`
6. **Plugins Folder**: `plugins/`

#### 2.3 Configurar VPC e Security Groups
- **VPC**: Use VPC existente ou crie nova
- **Subnets**: Pelo menos 2 subnets privadas
- **Security Groups**: Permitir HTTPS (443) e SSH (22)

### Passo 3: Configurar DBT

#### 3.1 Estrutura do Projeto DBT
```yaml
# my_dbt_project/dbt_project.yml
name: 'lpsdata_pipeline'
version: '1.0.0'
config-version: 2

profile: 'lpsdata'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"

models:
  lpsdata_pipeline:
    staging:
      +materialized: view
    marts:
      +materialized: table
```

#### 3.2 Configurar Profiles
```yaml
# .dbt/profiles.yml
lpsdata:
  target: prod
  outputs:
    prod:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
      database: "{{ env_var('SNOWFLAKE_DATABASE') }}"
      schema: "{{ env_var('SNOWFLAKE_SCHEMA') }}"
      role: "{{ env_var('SNOWFLAKE_ROLE', 'ACCOUNTADMIN') }}"
      threads: 4
      client_session_keep_alive: False
      query_tag: "dbt_mwaa_production"
```

### Passo 4: Configurar CI/CD

#### 4.1 GitHub Actions Workflow
O arquivo `.github/workflows/deploy-mwaa.yml` j√° foi criado e inclui:

- **Valida√ß√£o**: Testa DAGs e DBT
- **Deploy**: Upload para S3
- **Integra√ß√£o**: Atualiza MWAA
- **Testes**: Valida deployment

#### 4.2 Configurar Branch Protection
1. V√° em **Settings** ‚Üí **Branches**
2. Adicione regra para `main`:
   - Require pull request reviews
   - Require status checks to pass
   - Require branches to be up to date

### Passo 5: Configurar Monitoramento

#### 5.1 CloudWatch Alarms
```bash
# Alarm para falhas de DAG
aws cloudwatch put-metric-alarm \
  --alarm-name "MWAA-DAG-Failures" \
  --alarm-description "Alarm when DAGs fail" \
  --metric-name "DAGProcessingTime" \
  --namespace "AWS/MWAA" \
  --statistic "Average" \
  --period 300 \
  --threshold 1 \
  --comparison-operator "GreaterThanThreshold"
```

#### 5.2 SNS Notifications
```bash
# Criar t√≥pico SNS
aws sns create-topic --name "mwaa-alerts"

# Criar subscription
aws sns subscribe \
  --topic-arn "arn:aws:sns:us-east-1:123456789012:mwaa-alerts" \
  --protocol "email" \
  --notification-endpoint "your-email@example.com"
```

## üîß Como Funciona na Pr√°tica

### Cen√°rio: Pipeline Di√°rio de Dados

#### 1. **Desenvolvimento**
```bash
# Desenvolver novo modelo DBT
cd my_dbt_project/models/marts/
# Criar arquivo user_analytics.sql
# Fazer commit e push
```

#### 2. **CI/CD**
- GitHub Actions detecta mudan√ßas
- Valida sintaxe do DBT
- Testa conectividade
- Faz deploy para S3

#### 3. **Execu√ß√£o no MWAA**
```python
# DAG executa automaticamente √†s 6h UTC
@task
def run_dbt_build():
    # Executa: dbt build --target prod
    # Conecta no Snowflake
    # Executa SQL dos modelos
    # Salva resultados
```

#### 4. **Monitoramento**
- Logs no CloudWatch
- M√©tricas de performance
- Alertas por email
- Dashboard no Airflow UI

## üìä Exemplo de Pipeline Completo

### DAG de Produ√ß√£o
```python
# airflow-mwaa/dags/daily_data_pipeline.py
with DAG(
    dag_id="daily_data_pipeline",
    schedule="0 6 * * *",  # Di√°rio √†s 6h UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["production", "daily"],
) as dag:

    # Task 1: Validar ambiente
    validate = validate_environment()

    # Task 2: Executar DBT
    dbt_build = run_dbt_build()

    # Task 3: Enviar notifica√ß√£o
    notify = send_success_notification()

    validate >> dbt_build >> notify
```

### Modelo DBT
```sql
-- my_dbt_project/models/marts/user_analytics.sql
{{ config(materialized='table') }}

with user_events as (
    select * from {{ ref('staging_user_events') }}
),

user_metrics as (
    select
        user_id,
        count(*) as total_events,
        count(distinct event_date) as active_days,
        max(event_date) as last_activity
    from user_events
    group by user_id
)

select * from user_metrics
```

## üö® Troubleshooting

### Problemas Comuns

#### 1. **DAGs n√£o aparecem no MWAA**
```bash
# Verificar S3
aws s3 ls s3://lpsdata-airflow-1/dags/

# Verificar logs do MWAA
aws logs describe-log-groups --log-group-name-prefix "/aws/mwaa"
```

#### 2. **Erro de conectividade DBT**
```bash
# Verificar profiles
dbt debug --profiles-dir .dbt

# Testar conex√£o
dbt run --profiles-dir .dbt --target prod
```

#### 3. **Falha no CI/CD**
```bash
# Verificar logs do GitHub Actions
# Verificar secrets configurados
# Verificar permiss√µes AWS
```

## üìà Otimiza√ß√µes de Produ√ß√£o

### 1. **Performance**
- Use `dbt build` em vez de `dbt run` + `dbt test`
- Configure materializa√ß√µes apropriadas
- Use incremental models para dados grandes

### 2. **Custos**
- Configure schedule otimizado
- Use inst√¢ncias menores para desenvolvimento
- Monitore uso de recursos

### 3. **Seguran√ßa**
- Use Secrets Manager para credenciais
- Configure IAM roles com menor privil√©gio
- Habilite logging e auditoria

## üéØ Pr√≥ximos Passos

1. **Implementar**: Seguir este guia passo a passo
2. **Testar**: Executar pipeline de teste
3. **Monitorar**: Configurar alertas e dashboards
4. **Otimizar**: Ajustar baseado no uso real
5. **Escalar**: Adicionar mais pipelines conforme necess√°rio

## üìû Suporte

Se tiver d√∫vidas ou problemas:
1. Verifique os logs no CloudWatch
2. Consulte a documenta√ß√£o do MWAA
3. Teste localmente primeiro
4. Use o ambiente de desenvolvimento para debugging

---

**üéâ Parab√©ns!** Voc√™ agora tem um pipeline de dados completo em produ√ß√£o usando as melhores pr√°ticas da ind√∫stria!
